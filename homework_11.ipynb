{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b426105",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b78e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\howto\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\howto\\anaconda3\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: sklearn in c:\\users\\howto\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\howto\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\howto\\anaconda3\\lib\\site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers matplotlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47e6c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c7c227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=d750244aed9edd308a8a78c6c1235c4a51c355adc2c46eab6c9410877b58bbbc\n",
      "  Stored in directory: c:\\users\\howto\\appdata\\local\\pip\\cache\\wheels\\bd\\a8\\c3\\3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62133a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under opus.en-ru-train.ru\n",
      "\n",
      "Saved under opus.en-ru-train.en\n",
      "\n",
      "Saved under opus.en-ru-test.ru\n",
      "\n",
      "Saved under opus.en-ru-test.en\n"
     ]
    }
   ],
   "source": [
    "!python -m wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
    "!python -m wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
    "!python -m wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
    "!python -m wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6efaf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents = open('opus.en-ru-train.en', encoding='utf-8').read().lower().splitlines()\n",
    "ru_sents = open('opus.en-ru-train.ru', encoding='utf-8').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5102bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(WordPiece(), )\n",
    "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_en = WordPieceTrainer(\n",
    "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
    "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en )\n",
    "\n",
    "tokenizer_ru = Tokenizer(WordPiece(), )\n",
    "tokenizer_ru.normalizer = normalizers.Sequence([Lowercase()])\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_ru = WordPieceTrainer(\n",
    "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
    "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7438a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.decoder = decoders.WordPiece()\n",
    "tokenizer_ru.decoder = decoders.WordPiece()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "276b2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, target=False):\n",
    "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids + [tokenizer.token_to_id('[SEP]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d1998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, target=True) for t in ru_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9990466",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_en, max_len_ru = 20, 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec193beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c75fc0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_en = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              X_en, maxlen=max_len_en, padding='post', value=tokenizer_en.token_to_id('[PAD]'))\n",
    "\n",
    "X_ru_out = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              [x[1:] for x in X_ru], maxlen=max_len_ru-1, padding='post', \n",
    "              value=tokenizer_en.token_to_id('[PAD]'))\n",
    "\n",
    "X_ru_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              [x[:-1] for x in X_ru], maxlen=max_len_ru-1, \n",
    "              padding='post', value=tokenizer_ru.token_to_id('[PAD]'))\n",
    "\n",
    "X_en_train, X_en_valid, X_ru_dec_train, X_ru_dec_valid, X_ru_out_train, X_ru_out_valid = train_test_split(X_en, \n",
    "                                                                                                      X_ru_dec, \n",
    "                                                                                                      X_ru_out, \n",
    "                                                                                                      test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3124444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59b69193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # concatenation of heads\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb9e7031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5eddddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2741cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4b6144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb48ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            max_len,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28d0026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5babdbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            max_len,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6931d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                max_len,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "    # mask the future tokens for decoder inputs at the 1st attention block\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "    # mask the encoder outputs for the 2nd attention block\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "      vocab_size=vocab_size[0],\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      max_len=max_len[0],\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "      vocab_size=vocab_size[1],\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      max_len=max_len[1],\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1374ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none',)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    loss = L(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c28707fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "      def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2187254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# small model\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "\n",
    "# average model\n",
    "# NUM_LAYERS = 6\n",
    "# D_MODEL = 512\n",
    "# NUM_HEADS = 8\n",
    "# UNITS = 2048\n",
    "# DROPOUT = 0.1\n",
    "\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    model = transformer(\n",
    "        vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_ru.get_vocab_size()),\n",
    "        num_layers=NUM_LAYERS,\n",
    "        units=UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        max_len=[max_len_en, max_len_ru])\n",
    "\n",
    "#     learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "#         y_true = tf.reshape(y_true, shape=(-1, max_len_ru - 1))\n",
    "        return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('model_ruen',\n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=1,\n",
    "                                            save_weights_only=True,\n",
    "                                            save_best_only=True,\n",
    "                                            mode='min',\n",
    "                                            save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit((X_en_train, X_ru_dec_train), X_ru_out_train, \n",
    "             validation_data=((X_en_valid, X_ru_dec_valid), X_ru_out_valid),\n",
    "             batch_size=200,\n",
    "             epochs=5,\n",
    "             callbacks=[checkpoint]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1729125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text_batches):\n",
    "    sentences = []\n",
    "    \n",
    "    for batch in text_batches:\n",
    "        # Для каждого \"батча\" формируем вектор входных токенов из N предложений\n",
    "        batches = []\n",
    "        \n",
    "        for text in batch:\n",
    "            input_ids = encode(text.lower(), tokenizer_en)\n",
    "            batches.append(input_ids)\n",
    "            \n",
    "        input_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                                        batches, maxlen=max_len_en, padding='post')\n",
    "\n",
    "\n",
    "        # Формируем вектор выходных токенов с размерностью N\n",
    "        output_ids = [[tokenizer_ru.token_to_id('[CLS]') ]]*input_ids.shape[0]\n",
    "\n",
    "        # Предсказываем батчами\n",
    "        preds = model.predict((input_ids, tf.cast(output_ids, tf.int32)), batch_size=256)\n",
    "\n",
    "        # Получаем предсказания по всем токенам\n",
    "        pred = preds.argmax(2)[:, -1]\n",
    "\n",
    "        # Получаем матрицу из предсказаний до тех пор, пока не достигнем максимальной длины предложения на русском\n",
    "        for i in range(max_len_ru - 1):\n",
    "            # Добавляем в матрицу столбец с предсказаниями\n",
    "            output_ids = np.c_[output_ids, pred]\n",
    "            pred = model.predict((input_ids, tf.cast(output_ids, tf.int32)), batch_size=64).argmax(2)[:, -1]\n",
    "            \n",
    "            sentences.extend(output_ids)\n",
    "  \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b079662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 242ms/step\n",
      "16/16 [==============================] - 4s 77ms/step\n",
      "16/16 [==============================] - 2s 89ms/step\n",
      "16/16 [==============================] - 2s 124ms/step\n",
      "16/16 [==============================] - 3s 159ms/step\n",
      "16/16 [==============================] - 2s 140ms/step\n",
      "16/16 [==============================] - 3s 155ms/step\n",
      "16/16 [==============================] - 3s 171ms/step\n",
      "16/16 [==============================] - 3s 179ms/step\n",
      "16/16 [==============================] - 3s 212ms/step\n",
      "16/16 [==============================] - 4s 262ms/step\n",
      "16/16 [==============================] - 4s 254ms/step\n",
      "16/16 [==============================] - 5s 294ms/step\n",
      "16/16 [==============================] - 5s 327ms/step\n",
      "16/16 [==============================] - 5s 310ms/step\n",
      "16/16 [==============================] - 6s 380ms/step\n",
      "16/16 [==============================] - 7s 402ms/step\n",
      "16/16 [==============================] - 6s 346ms/step\n",
      "16/16 [==============================] - 7s 376ms/step\n",
      "16/16 [==============================] - 7s 379ms/step\n",
      "16/16 [==============================] - 7s 423ms/step\n",
      "16/16 [==============================] - 6s 369ms/step\n",
      "4/4 [==============================] - 2s 267ms/step\n",
      "16/16 [==============================] - 1s 82ms/step\n",
      "16/16 [==============================] - 2s 109ms/step\n",
      "16/16 [==============================] - 2s 137ms/step\n",
      "16/16 [==============================] - 2s 124ms/step\n",
      "16/16 [==============================] - 2s 138ms/step\n",
      "16/16 [==============================] - 2s 145ms/step\n",
      "16/16 [==============================] - 3s 160ms/step\n",
      "16/16 [==============================] - 3s 164ms/step\n",
      "16/16 [==============================] - 3s 213ms/step\n",
      "16/16 [==============================] - 3s 215ms/step\n",
      "16/16 [==============================] - 4s 252ms/step\n",
      "16/16 [==============================] - 4s 235ms/step\n",
      "16/16 [==============================] - 4s 258ms/step\n",
      "16/16 [==============================] - 5s 282ms/step\n",
      "16/16 [==============================] - 5s 302ms/step\n",
      "16/16 [==============================] - 5s 316ms/step\n",
      "16/16 [==============================] - 6s 328ms/step\n",
      "16/16 [==============================] - 5s 306ms/step\n",
      "16/16 [==============================] - 5s 301ms/step\n",
      "16/16 [==============================] - 5s 310ms/step\n",
      "16/16 [==============================] - 5s 312ms/step\n"
     ]
    }
   ],
   "source": [
    "sentences = translate(text_batches)\n",
    "\n",
    "sep_token = tokenizer_ru.token_to_id('[SEP]')\n",
    "\n",
    "sentences = [sentence[1:-1 if len(np.where(sentence==sep_token)[0]) == 0 else np.where(sentence==sep_token)[0][0]] for sentence in sentences]\n",
    "sentences = [tokenizer_ru.decode(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a00a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "bleus = []\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    reference = tokenizer_ru.encode(sentence).tokens\n",
    "    hypothesis = tokenizer_ru.encode(ru_sents_test[i]).tokens\n",
    "\n",
    "bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b25f3e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(bleus)/len(bleus))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08444fa",
   "metadata": {},
   "source": [
    "???? (маленькое количество эпох?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1543859",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe750a6",
   "metadata": {},
   "source": [
    "Обратный перевод — это способ использования одноязычных корпусов на целевом языке путем создания синтетических би-текстов. \n",
    "\n",
    "В основном обратный перевод используется на малых одноязычных данных; пример, приведённый в статье, рассказывает об обратном переводе новахо-английских текстов, когда у исследователей имеется лишь небольшой текст, хотя достаточное количество англоязычных данных. Перевод одноязычного английского текста на навахо позволит добавить этот синтетический би-текст навахо/английский к дргим обучающим данным.\n",
    "\n",
    "*Для пары en-ru:*\n",
    "\n",
    "<p>Берём параллельный корпус и переводим его с ру на англ;<br>\n",
    "    Добавляем полученный синтетический параллельный корпус к обучающим данным;<br>\n",
    "    На полученном \"словаре\" обучаем модель на перевод с английского на русский язык </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
