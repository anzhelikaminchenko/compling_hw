{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbdf9b28",
   "metadata": {},
   "source": [
    "# Задание № 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fded281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a29af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92642831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "dvach = open('2ch_corpus.txt', encoding = 'UTF-8').read()\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "sentences_dvach = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach)][:-100]\n",
    "sentences_perplexity = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach)][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81a6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "    trigrams_dvach.update(ngrammer(sentence, n=3))\n",
    "    if i >4000:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2996d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = np.zeros((len(bigrams_dvach), \n",
    "                   len(unigrams_dvach)))\n",
    "\n",
    "id2word_dvach_1 = list(unigrams_dvach)\n",
    "word2id_dvach_1 = {word:i for i, word in enumerate(id2word_dvach_1)}\n",
    "\n",
    "id2word_dvach_2 = list(bigrams_dvach)\n",
    "word2id_dvach_2 = {word:i for i, word in enumerate(id2word_dvach_2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba3897fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef75006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dvach = lil_matrix((len(bigrams_dvach), \n",
    "                   len(unigrams_dvach)))\n",
    "\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "\n",
    "id2bigram_dvach = list(bigrams_dvach)\n",
    "bigram2id_dvach = {word:i for i, word in enumerate(id2bigram_dvach)}\n",
    "\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' ' + word2\n",
    "\n",
    "    matrix_dvach[bigram2id_dvach[bigram], word2id_dvach[word3]] =  (trigrams_dvach[ngram]/\n",
    "                                                                  bigrams_dvach[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfa54caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2bigram, id2word, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        p = matrix[current_idx].toarray()[0]\n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])), p = p)\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            current_idx = bigram2id[start]\n",
    "            \n",
    "        else:\n",
    "            part = id2bigram[current_idx] + ' ' + id2word[chosen]\n",
    "            part = ' '.join(part.split()[1:])\n",
    "            current_idx = bigram2id[part]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0acf874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я не люблю людей но я не ем в макадаках бургеркингах и т п спеасибо анон каждый раз пить перед встречей как то шарпер ide а там нормальная зарплата \n",
      "\n",
      " на 1 раз \n",
      "\n",
      " видя неэффективность пуль сая просается в банзай-атаку порубав мутанта на мелкие кусочки \n",
      "\n",
      " decapitated я их вообще ни разу не слышал или какой-то ультраобскур типа mercury clean j и т нисколько \n",
      "\n",
      " тогда не знаю \n",
      "\n",
      " так что анон не знаю как это отразилось на моей жизни \n",
      "\n",
      " еще кто то ведется и пишет кошельки \n",
      "\n",
      " в летней компьютерной школе она не задрот \n",
      "\n",
      " это хорошие\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2bigram_dvach, id2word_dvach, bigram2id_dvach).replace('<end>', '\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3da1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    \n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "697a854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> <start> ты не угадал немного <end> 5253.58080933882 \n",
      "\n",
      "<start> <start> я не сельдь ни разу <end> 5673.477101658685 \n",
      "\n",
      "<start> <start> аноны поясните как делать куни <end> 58485.22711218058 \n",
      "\n",
      "<start> <start> что там надо лизать <end> 5253.58080933882 \n",
      "\n",
      "<start> <start> предстоит встреча с тней но я хз что делать <end> 35294.913792436404 \n",
      "\n",
      "<start> <start> ебался всего один раз со шлюхой 20 лвл мимогей сосал членик только 3 олсо шлюхе не рисковал бы отлизать <end> 100000.00000000026 \n",
      "\n",
      "<start> <start> просто включи немного альфачества сходи в клуб найди тян на работе учебе поверь это не так сложно <end> 44930.25112041807 \n",
      "\n",
      "<start> <start> крч делал дохуя раз двум тянкам не нравился куник <end> 99999.99999999997 \n",
      "\n",
      "<start> <start> хотя сам замечал что второй норм было <end> 50816.44995064175 \n",
      "\n",
      "<start> <start> после куни попизже трахаться ибо тян больше возбуждается и влагалище более чувствительное становится <end> 69916.61868022686 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences_perplexity[:10]:\n",
    "    prob = []\n",
    "    \n",
    "    for ngram in ngrammer(sent, 3):\n",
    "        bigram = \" \".join(ngram.split()[:2])\n",
    "\n",
    "        if ngram in trigrams_dvach and bigram in bigrams_dvach:\n",
    "            prob.append(np.log(trigrams_dvach[ngram] / bigrams_dvach[bigram]))\n",
    "            \n",
    "        else:\n",
    "            prob.append(np.log(0.00001))\n",
    "            \n",
    "    print(\" \".join(sent), perplexity(prob), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad06a4",
   "metadata": {},
   "source": [
    "# Задание № 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b4b42f",
   "metadata": {},
   "source": [
    "1) Есть два распространенных способа тренировать вероятности неизвестного словарю слова (<UNK>). Первый - превратить проблему обратно в проблему с закрытым словарным запасом, заранее выбрав фиксированный словарный запас: \n",
    "1. Выбрать подготовленный словарь (список слов) (выбрать фиксированный словарь). \n",
    "2. Преобразовать в обучающем наборе любое слово, не входящее в этот набор (OOV), в the unknown word token (<UNK>) на шаге нормализации текста.\n",
    "3. Оценить вероятности подученных токенов как и любых других слов в обучающем наборе. \n",
    "Вторая альтернатива, в ситуациях, когда нет предварительного фиксированного словаря, заключается в создании такого словаря неявно, заменяя слова в обучающих данных на <UNK>, основываясь на их частоте. (С помощью вычисления частоты (а именно - малочастотности) заменять их на UNK.)\n",
    "\n",
    "2) Чтобы словам, которые имеются в слоаре, но находятся в тестовом наборе в невидимом контексте (например, они появляются после слова, после которого они никогда не появлялись при обучении), не приписывалась нулевая вероятность, им добавляется какая-то часть вероятности более частотных слов. Эта модификация называется сглаживанием или дисконтированием (smoothing or discounting).\n",
    "   Существует несколько типов сглаживания:\n",
    "1. Сглаживание Лапласа (самы простой вариант) - добавить единицу ко всем счетчикам n-грамм, прежде чем нормировать их в вероятности.\n",
    "2. Сглаживание add-k - сдвинуть немного меньше вероятности видимых контекстов к невидимым.\n",
    "3. Backoff-сглаживание - использование меньшего количества контекста (вероятность (N-1)-грамм).\n",
    "4. Сглаживание Кнезера-Нея "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
