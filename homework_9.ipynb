{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b13068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\howto\\anaconda3\\lib\\site-packages (1.2.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\howto\\anaconda3\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\howto\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\howto\\anaconda3\\lib\\site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\howto\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\howto\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\howto\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\howto\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=07942321bdd60c4ad4ad47acaa5e1c3694c9c28b51c0a61e8a4e7af7dea444ea\n",
      "  Stored in directory: c:\\users\\howto\\appdata\\local\\pip\\cache\\wheels\\22\\0b\\40\\fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: tokenizers, transformers, sklearn\n",
      "Successfully installed sklearn-0.0 tokenizers-0.12.1 transformers-4.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sklearn pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b0e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca68478",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('lenta_sample.csv')\n",
    "data.dropna(subset=['topic', 'text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5296722",
   "metadata": {},
   "source": [
    "#### bert-base-multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30265abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb6c9eeff0a4a1bb9d77616e5c6e735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0577f7a0db842b1b45e0e437186d3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/953M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5a3af009894dc9b90270e8172a96f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb703232fca4227bf81a21ba79f38b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1798c7d0fdd54c21899450ba10d7eea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.64M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert = TFAutoModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7938e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for text in data.title:\n",
    "    ids = tokenizer.encode(text)\n",
    "    X.append(ids[:512])\n",
    "\n",
    "id2label = {i:label for i, label in enumerate(set(data.topic.values))}\n",
    "label2id = {l:i for i, l in id2label.items()}   \n",
    "    \n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='post', maxlen=512)\n",
    "y = tf.keras.utils.to_categorical([label2id[label] for label in data.topic.values])\n",
    "\n",
    "train_index, valid_index = train_test_split(list(range(len(X))), test_size=0.05, stratify=data.topic)\n",
    "\n",
    "X_train, y_train = X[train_index], y[train_index]\n",
    "X_valid, y_valid = X[valid_index], y[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fbb1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32,\n",
    "                                     name=\"input_ids\")\n",
    "\n",
    "output = bert({\"input_ids\":input_word_ids})\n",
    "drop = tf.keras.layers.Dropout(0.3)(output[0][:, 0])\n",
    "dense = tf.keras.layers.Dense(y.shape[1], activation='softmax')(drop)\n",
    "\n",
    "bert_clf = tf.keras.Model(inputs=input_word_ids, outputs=dense)\n",
    "\n",
    "bert_clf.compile(tf.optimizers.Adam(learning_rate=2e-6,\n",
    "                                    ), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy', \n",
    "                           tf.keras.metrics.RecallAtPrecision(0.80)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid),\n",
    "          batch_size=2,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9495f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b856c9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 48s 6s/step\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         Культура       0.50      0.33      0.40         3\n",
      "           Россия       0.00      0.00      0.00         2\n",
      "           Бизнес       0.00      0.00      0.00         2\n",
      "        Экономика       0.00      0.00      0.00         2\n",
      "  Наука и техника       0.10      0.67      0.17         3\n",
      "   69-я параллель       0.00      0.00      0.00         1\n",
      "      Бывший СССР       0.00      0.00      0.00         3\n",
      "             Крым       0.00      0.00      0.00         0\n",
      "          Легпром       0.00      0.00      0.00         1\n",
      "              Дом       0.00      0.00      0.00         2\n",
      "   Интернет и СМИ       0.00      0.00      0.00         2\n",
      "         Из жизни       0.22      0.67      0.33         3\n",
      "            Спорт       0.00      0.00      0.00         2\n",
      "       Библиотека       0.00      0.00      0.00         0\n",
      "         Ценности       0.00      0.00      0.00         2\n",
      "              Мир       0.00      0.00      0.00         1\n",
      "Силовые структуры       0.00      0.00      0.00         2\n",
      "\n",
      "        micro avg       0.16      0.16      0.16        31\n",
      "        macro avg       0.05      0.10      0.05        31\n",
      "     weighted avg       0.08      0.16      0.09        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = bert_clf.predict(X_valid, batch_size=5).argmax(1)\n",
    "print(classification_report(y_valid.argmax(1), pred, labels=list(range(len(label2id))),\n",
    "                            target_names=list(label2id), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081687f",
   "metadata": {},
   "source": [
    "##### roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f50cb06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5984755d748944e7a7bb9d85213f811d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9f8c1c16934efb851af9285ee112de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6e3b32d33b479a9be31e9fdd39b576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099dcb4955a2492caf5fa41f53d46e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_roberta = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b88e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for text in data.title:\n",
    "    ids = tokenizer_roberta.encode(text)\n",
    "    \n",
    "    X.append(ids[:512])\n",
    "    \n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='post', maxlen=512)\n",
    "\n",
    "id2label = {i:label for i, label in enumerate(set(data.topic.values))}\n",
    "label2id = {l:i for i, l in id2label.items()}\n",
    "\n",
    "y = tf.keras.utils.to_categorical([label2id[label] for label in data.topic.values])\n",
    "\n",
    "train_index, valid_index = train_test_split(list(range(len(X))), test_size=0.05, stratify=data.topic)\n",
    "\n",
    "X_train, y_train = X[train_index], y[train_index]\n",
    "X_valid, y_valid = X[valid_index], y[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecb00358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d521c21b93b245ad90ebab1b54c54536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/627M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "roberta = TFAutoModel.from_pretrained('roberta-base', trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64fa1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32,\n",
    "                                     name=\"input_ids\")\n",
    "\n",
    "output = roberta({\"input_ids\":input_word_ids})\n",
    "drop = tf.keras.layers.Dropout(0.3)(output[0][:, 0]) \n",
    "dense = tf.keras.layers.Dense(y.shape[1], activation='softmax')(drop)\n",
    "\n",
    "roberta_clf = tf.keras.Model(inputs=input_word_ids, outputs=dense)\n",
    "\n",
    "\n",
    "roberta_clf.compile(tf.optimizers.Adam(learning_rate=2e-6,\n",
    "                                    ), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy', \n",
    "                           tf.keras.metrics.RecallAtPrecision(0.80, name='rec_prec')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b14182",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_clf.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid),\n",
    "          batch_size=2,\n",
    "          epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d6b9fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 41s 6s/step\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         Культура       0.00      0.00      0.00         3\n",
      "           Россия       0.00      0.00      0.00         2\n",
      "           Бизнес       0.00      0.00      0.00         2\n",
      "        Экономика       0.00      0.00      0.00         2\n",
      "  Наука и техника       0.10      1.00      0.18         3\n",
      "   69-я параллель       0.00      0.00      0.00         1\n",
      "      Бывший СССР       0.00      0.00      0.00         3\n",
      "             Крым       0.00      0.00      0.00         0\n",
      "          Легпром       0.00      0.00      0.00         1\n",
      "              Дом       0.00      0.00      0.00         2\n",
      "   Интернет и СМИ       0.00      0.00      0.00         2\n",
      "         Из жизни       0.00      0.00      0.00         3\n",
      "            Спорт       0.00      0.00      0.00         2\n",
      "       Библиотека       0.00      0.00      0.00         0\n",
      "         Ценности       0.00      0.00      0.00         2\n",
      "              Мир       0.00      0.00      0.00         1\n",
      "Силовые структуры       0.00      0.00      0.00         2\n",
      "\n",
      "        micro avg       0.10      0.10      0.10        31\n",
      "        macro avg       0.01      0.06      0.01        31\n",
      "     weighted avg       0.01      0.10      0.02        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = roberta_clf.predict(X_valid, batch_size=5).argmax(1)\n",
    "print(classification_report(y_valid.argmax(1), pred, labels=list(range(len(label2id))),\n",
    "                            target_names=list(label2id), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51db666",
   "metadata": {},
   "source": [
    "#### camembert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f2ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at camembert-base were not used when initializing TFCamembertModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFCamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFCamembertModel were not initialized from the model checkpoint at camembert-base and are newly initialized: ['roberta/pooler/dense/kernel:0', 'roberta/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "camembert = TFAutoModel.from_pretrained('camembert-base', trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3eda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for text in data.title:\n",
    "    ids = tokenizer_camembert.encode(text)\n",
    "    X.append(ids[:512])\n",
    "\n",
    "id2label = {i:label for i, label in enumerate(set(data.topic.values))}\n",
    "label2id = {l:i for i, l in id2label.items()}   \n",
    "    \n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='post', maxlen=512)\n",
    "y = tf.keras.utils.to_categorical([label2id[label] for label in data.topic.values])\n",
    "\n",
    "train_index, valid_index = train_test_split(list(range(len(X))), test_size=0.05, stratify=data.topic)\n",
    "\n",
    "X_train, y_train = X[train_index], y[train_index]\n",
    "X_valid, y_valid = X[valid_index], y[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d1f8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32,\n",
    "                                     name=\"input_ids\")\n",
    "\n",
    "output = camembert({\"input_ids\":input_word_ids})\n",
    "drop = tf.keras.layers.Dropout(0.3)(output[0][:, 0])\n",
    "dense = tf.keras.layers.Dense(y.shape[1], activation='softmax')(drop)\n",
    "\n",
    "camembert_clf = tf.keras.Model(inputs=input_word_ids, outputs=dense)\n",
    "\n",
    "camembert_clf.compile(tf.optimizers.Adam(learning_rate=2e-6,\n",
    "                                    ), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy', \n",
    "                           tf.keras.metrics.RecallAtPrecision(0.80)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b75c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "camembert_clf.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid),\n",
    "          batch_size=3,\n",
    "          epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb7c90bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 70s 10s/step\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "   69-я параллель       0.00      0.00      0.00         1\n",
      "        Экономика       0.00      0.00      0.00         2\n",
      "       Библиотека       0.00      0.00      0.00         0\n",
      "            Спорт       0.00      0.00      0.00         2\n",
      "           Бизнес       0.00      0.00      0.00         2\n",
      "  Наука и техника       0.00      0.00      0.00         3\n",
      "      Бывший СССР       0.00      0.00      0.00         3\n",
      "         Культура       0.10      1.00      0.18         3\n",
      "             Крым       0.00      0.00      0.00         0\n",
      "         Из жизни       0.00      0.00      0.00         3\n",
      "Силовые структуры       0.00      0.00      0.00         2\n",
      "              Дом       0.00      0.00      0.00         2\n",
      "              Мир       0.00      0.00      0.00         1\n",
      "   Интернет и СМИ       0.00      0.00      0.00         2\n",
      "         Ценности       0.00      0.00      0.00         2\n",
      "           Россия       0.00      0.00      0.00         2\n",
      "          Легпром       0.00      0.00      0.00         1\n",
      "\n",
      "        micro avg       0.10      0.10      0.10        31\n",
      "        macro avg       0.01      0.06      0.01        31\n",
      "     weighted avg       0.01      0.10      0.02        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = camembert_clf.predict(X_valid, batch_size=5).argmax(1)\n",
    "print(classification_report(y_valid.argmax(1), pred, labels=list(range(len(label2id))),\n",
    "                            target_names=list(label2id), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45750b64",
   "metadata": {},
   "source": [
    "**результаты на всех моделях почти одинаково плохи (возможно, недообучены из-за неполного количества эпох);**\n",
    "\n",
    "**из полученных результатов лучше справилась модель bert**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9877e",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935fa785",
   "metadata": {},
   "source": [
    "1) Модель BERT обучается с помощью двух разных задач: модели маскированного языка (MLM) и прогнозирования следующего предложения (NSP). \n",
    "\n",
    "RoBERTa является расширением модели Bert (оптимизированная и надежная версия BERT).\n",
    "\n",
    "Модель RoBERTa обучается с большими партиями и более длинными последовательностями; это повышает точность конечной задачи по сравнению с BERT.\n",
    "\n",
    "Когда данные передаются в модель RoBERTa, каждый раз выполняются разные стратегии маскирования. Модель BERT, с другой стороны, использует стратегию статического маскирования и выполняет маскирование только во время предварительной обработки данных .\n",
    "\n",
    "Исходя из этого, делается вывод, что roberta лучше bert (хотя обучается дольше)\n",
    "\n",
    "ссылки: https://arxiv.org/pdf/1907.11692.pdf;\n",
    " https://www.sciencedirect.com/science/article/pii/S0306457321002375\n",
    "        \n",
    "2) Модель T5 – это уже обученная многозадачная нейросеть: может хорошо понимать и генерировать текст. \n",
    "Модель можно дообучить на собственную задачу (например, генерация ответа, суммаризация текстов и даже перевод).\n",
    "В отличие от bert t5 можно декодировать на свой вкус, поэтому эта модель многозадачнее."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
